{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cb0424",
   "metadata": {},
   "source": [
    "# Modelagem Supervisionada - Previsão de Churn\n",
    "\n",
    "Neste notebook, treinamos e avaliamos modelos supervisionados para prever o cancelamento de clientes (churn) com base nos dados processados a partir do dataset da Telco.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f407192",
   "metadata": {},
   "source": [
    "## Importação das funções e dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import load_data, preprocess_data, split_and_transform\n",
    "from src.modeling import train_models, evaluate_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be2f71",
   "metadata": {},
   "source": [
    "### Carregando e preparando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brendon/Documentos/Projetos/Projeto1/src/preprocessing.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = load_data()\n",
    "X, y, preprocessor = preprocess_data(df)\n",
    "X_train, X_test, y_train, y_test = split_and_transform(X, y, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b739b8",
   "metadata": {},
   "source": [
    "A seguir, aplicamos o pré-processamento definido anteriormente: encoding, imputação e scaling. Depois dividimos em treino/teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, preprocessor = preprocess_data(df)\n",
    "X_train, X_test, y_train, y_test = split_and_transform(X, y, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5281e49b",
   "metadata": {},
   "source": [
    "## Treinamento dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e5ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modeling import train_models, evaluate_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brendon/Documentos/Projetos/Projeto1/venv/lib/python3.13/site-packages/xgboost/training.py:183: UserWarning: [12:41:37] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "models = train_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7415b83",
   "metadata": {},
   "source": [
    "## Avaliação de desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo: LogisticRegression\n",
      "Acurácia: 0.8088\n",
      "F1-score: 0.6085\n",
      "ROC AUC: 0.8447\n",
      "Matriz de confusão:\n",
      "[[1395  157]\n",
      " [ 247  314]]\n",
      "Relatório:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87      1552\n",
      "           1       0.67      0.56      0.61       561\n",
      "\n",
      "    accuracy                           0.81      2113\n",
      "   macro avg       0.76      0.73      0.74      2113\n",
      "weighted avg       0.80      0.81      0.80      2113\n",
      "\n",
      "\n",
      "Modelo: RandomForest\n",
      "Acurácia: 0.7899\n",
      "F1-score: 0.5578\n",
      "ROC AUC: 0.8197\n",
      "Matriz de confusão:\n",
      "[[1389  163]\n",
      " [ 281  280]]\n",
      "Relatório:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86      1552\n",
      "           1       0.63      0.50      0.56       561\n",
      "\n",
      "    accuracy                           0.79      2113\n",
      "   macro avg       0.73      0.70      0.71      2113\n",
      "weighted avg       0.78      0.79      0.78      2113\n",
      "\n",
      "\n",
      "Modelo: XGBoost\n",
      "Acurácia: 0.7818\n",
      "F1-score: 0.5572\n",
      "ROC AUC: 0.8164\n",
      "Matriz de confusão:\n",
      "[[1362  190]\n",
      " [ 271  290]]\n",
      "Relatório:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.86      1552\n",
      "           1       0.60      0.52      0.56       561\n",
      "\n",
      "    accuracy                           0.78      2113\n",
      "   macro avg       0.72      0.70      0.71      2113\n",
      "weighted avg       0.77      0.78      0.78      2113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_models(models, X_test, y_test)\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f'\\nModelo: {name}')\n",
    "    print(f\"Acurácia: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1-score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    print(\"Matriz de confusão:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(\"Relatório:\")\n",
    "    print(metrics['report'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffb967",
   "metadata": {},
   "source": [
    "Vamos visualizar os principais indicadores de cada modelo: acurácia, F1-score, ROC AUC e matriz de confusão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b206e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f'\\nModelo: {name}')\n",
    "    print(f\"Acurácia: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1-score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    print(\"Matriz de confusão:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    print(\"Relatório:\")\n",
    "    print(metrics['report'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac476c",
   "metadata": {},
   "source": [
    "## Salvando modelo e pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../app/preprocessor.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Supondo que LogisticRegression foi o melhor modelo\n",
    "melhor_modelo = models['LogisticRegression']\n",
    "\n",
    "# Salvar modelo treinado\n",
    "joblib.dump(melhor_modelo, '../app/model.pkl')\n",
    "\n",
    "# Salvar pipeline de pré-processamento\n",
    "joblib.dump(preprocessor, '../app/preprocessor.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d3d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Salvando o melhor modelo (LogisticRegression foi o melhor neste caso)\n",
    "import os\n",
    "joblib.dump(models['LogisticRegression'], os.path.join('app', 'model.pkl'))\n",
    "joblib.dump(preprocessor, os.path.join('app', 'preprocessor.pkl'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e788ecac",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "O modelo **Logistic Regression** apresentou o melhor desempenho em termos de equilíbrio entre F1-score e ROC AUC, e foi salvo para ser utilizado na aplicação.\n",
    "\n",
    "Na próxima etapa, esse modelo será carregado para fornecer previsões via API (FastAPI) e interface web (Streamlit).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
